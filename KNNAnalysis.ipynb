{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf027c97",
   "metadata": {},
   "source": [
    "# KNN Analysis for Test/Training Data (Patient S0 & Patient S1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696762e",
   "metadata": {},
   "source": [
    "Import Necessary Packages/Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "339f9d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import sklearn \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e41e354",
   "metadata": {},
   "source": [
    "### Functions to process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c37938c",
   "metadata": {},
   "source": [
    "create_rdr_baf(self) is a function that selects the columns of interest for RDR and BAF by concatenating the chromosome, start, and end numbers into a single column and pivoting the data from long to wide. After the RDR and BAF are pivoted individually, they are inner-merged to create a new dataframe. This allows for easier analysis as the cell sequence is identified as the index with RDR and BAF inputs for the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "0fc61999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rdr_baf(self):\n",
    "    print(self['CELL'].unique().shape[0])\n",
    "    crdr = self[['#CHR', 'START', 'END', 'CELL', 'RDR']] #Selects columns of interest for RDR\n",
    "    \n",
    "    pd.set_option('mode.chained_assignment', None) #Removes SettingwithCopyWarning \n",
    "\n",
    "    crdr['#CHR']= crdr['#CHR'].astype(str) \n",
    "    crdr['START']= crdr['START'].astype(str)\n",
    "    crdr['END']= crdr['END'].astype(str)\n",
    "\n",
    "    crdr[\"chr_start_end\"]= crdr['#CHR'] + '_' + crdr[\"START\"]+ '_' + crdr['END']  #Concatenates columns\n",
    "\n",
    "    crdr = crdr.pivot(index='CELL', columns='chr_start_end', values='RDR')   #Pivots data from long to wide\n",
    "    #print(crdr.shape)\n",
    "    baf = self[['#CHR', 'START', 'END', 'CELL', 'BAF']] #Selects columns of interest for BAF\n",
    "\n",
    "    pd.set_option('mode.chained_assignment', None) #Removes SettingwithCopyWarning \n",
    "\n",
    "    baf['#CHR']= baf['#CHR'].astype(str) \n",
    "    baf['START']= baf['START'].astype(str)\n",
    "    baf['END']= baf['END'].astype(str)\n",
    "\n",
    "    baf[\"chr_start_end\"]= baf['#CHR'] + '_' + baf[\"START\"]+ '_' + baf['END']  #Concatenates columns\n",
    "\n",
    "    baf = baf.pivot(index='CELL', columns='chr_start_end', values='BAF')   #Pivots data from long to wide\n",
    "    #print(baf.shape)\n",
    "    x = crdr.merge(baf, how='inner', on='CELL')  #Concatenates columns\n",
    "    #print(x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa8f66",
   "metadata": {},
   "source": [
    "pca_df(df) is a function that performs the task of dimensionality reduction, specifically into 2 dimensions, creating a new dataframe with 'pca1' and 'pca2' columns. This is a significant step that allows for better analysis of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "917e6372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_df(df):\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(df)\n",
    "    principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['pca1', 'pca2'])\n",
    "    return principalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d3adf",
   "metadata": {},
   "source": [
    "pcas_df(df, n) is a function that performs the task of dimensionality reduction, based on the input of dimensions. This function is different from pca_df() in the fact that it uses a fit method that allows the user to fit the input of the dataframe. It also returns this fit transformed dataframe. This is a significant function that allows for analysis of patient s0 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "a032c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcas_df(df, n):\n",
    "    pca = PCA(n_components=n)\n",
    "    #X = df.drop('CELL', axis=1)\n",
    "    \n",
    "    pcafit = pca.fit(df)\n",
    "    principalComponents = pcafit.transform(df)\n",
    "    return principalComponents, pcafit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cbb94f",
   "metadata": {},
   "source": [
    "reformat(r) is a function that allows us to adjust the structure of our rows and columns within the dataset. This step is crucial for further analysis and comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "1d475108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(r):\n",
    "    rnew = np.empty(r.shape, dtype='U25')\n",
    "    for i,val in enumerate(r):\n",
    "        new_string = f\"{val[0]}_{val[1]}_{val[2]}\"    \n",
    "        new_string = new_string.replace('chr', '')\n",
    "        rnew[i] = new_string\n",
    "    return rnew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf2d01",
   "metadata": {},
   "source": [
    "### Read in process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2df3a2",
   "metadata": {},
   "source": [
    "Read all the files from PatientS0 into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "549a44ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datA = pd.read_csv(\"patientS0/calls/sectionA/calls.tsv.gz\", sep=\"\\t\", compression=\"gzip\" )\n",
    "datB = pd.read_csv(\"patientS0/calls/sectionB/calls.tsv.gz\", sep=\"\\t\", compression=\"gzip\" )\n",
    "datC = pd.read_csv(\"patientS0/calls/sectionC/calls.tsv.gz\", sep=\"\\t\", compression=\"gzip\" )\n",
    "datD = pd.read_csv(\"patientS0/calls/sectionD/calls.tsv.gz\", sep=\"\\t\", compression=\"gzip\" )\n",
    "datE = pd.read_csv(\"patientS0/calls/sectionE/calls.tsv.gz\", sep=\"\\t\", compression=\"gzip\" )\n",
    "data = {'A': datA, 'B': datB,'C': datC,'D': datD,'E': datE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee9c4a",
   "metadata": {},
   "source": [
    "Read all the files from PatientS1 into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "8a39b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datA1 = pd.read_csv(\"patientS1/calls/sectionA1/calls.tsv.gz\", sep=\"\\t\", compression=\"gzip\" )\n",
    "datA2 = pd.read_csv(\"patientS1/calls/sectionA2/calls.tsv.gz\", sep=\"\\t\", compression=\"gzip\" )\n",
    "datA3 = pd.read_csv(\"patientS1/calls/sectionA3/calls.tsv.gz\", sep=\"\\t\", compression=\"gzip\" )\n",
    "datB1 = pd.read_csv(\"patientS1/calls/sectionB1/calls.tsv.gz\", sep=\"\\t\", compression=\"gzip\" )\n",
    "datB2 = pd.read_csv(\"patientS1/calls/sectionB2/calls.tsv.gz\", sep=\"\\t\", compression=\"gzip\" )\n",
    "datas = {'A1': datA1, 'A2': datA2,'A3': datA3,'B1': datB1,'B2': datB2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aedc67",
   "metadata": {},
   "source": [
    "Preprocess the Data with reformat() function and remove 'chr' within the #CHR column. Also, create an intersection to change the amount of columns within the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "1cc883ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    temp = data[d].copy()\n",
    "    temps = temp['#CHR'].str.replace('chr', '')\n",
    "    temp['#CHR'] = temps\n",
    "    data[d] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "d16fa49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas1 = pd.concat(datas)\n",
    "datas1 = datas1.set_index(['#CHR', 'START', 'END'])\n",
    "datas0 = pd.concat(data)\n",
    "datas0 = datas0.set_index(['#CHR', 'START', 'END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "a1a6ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1bins = datas1.index.unique().to_numpy()\n",
    "s0bins = datas0.index.unique().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "fbafc998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(544,)\n"
     ]
    }
   ],
   "source": [
    "reformats0 = reformat(s0bins)\n",
    "reformats1 = reformat(s1bins)\n",
    "intersect = np.intersect1d(reformats0, reformats1)\n",
    "print(intersect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44704c25",
   "metadata": {},
   "source": [
    "Convert the labels of the dataframe to different datatypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "7ff0091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(data = intersect \n",
    "             , columns = ['label'])\n",
    "labels[['#CHR','START', 'END']] = labels['label'].str.split('_',expand=True)\n",
    "labels = labels.drop(columns = ['label'])\n",
    "\n",
    "convert_dict = {'START' : int, 'END' : int}\n",
    "labels = labels.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd66dff",
   "metadata": {},
   "source": [
    "Preprocess the data first by inner-merging the labels into the data on #CHR, START, and END columns. Then, transform the data from wide to long with a column for each RDR and BAF and a row for a each cell. Use \"SECTION-BARCODE\" as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "e96b61b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2191\n",
      "2239\n",
      "1754\n",
      "1943\n",
      "2075\n",
      "total cells:10202\n"
     ]
    }
   ],
   "source": [
    "final = []\n",
    "total = 0\n",
    "for d in data:\n",
    "    data[d] = data[d].merge(labels, how='inner', on=['#CHR', 'START', 'END'])\n",
    "    x = create_rdr_baf(data[d])\n",
    "    cell_id = d + \"-\" + data[d]['CELL'].unique() \n",
    "    total += cell_id.shape[0]\n",
    "    x = x.reset_index()\n",
    "    x = x.drop(columns=['CELL'])\n",
    "    x['CELL'] = cell_id\n",
    "    x = x.set_index(\"CELL\")\n",
    "    final.append(x)\n",
    "print(\"total cells:\" + str(total))\n",
    "result_s0 = pd.concat(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d6861",
   "metadata": {},
   "source": [
    "Reduce the dimensionality of the data using PCA into 2 dimensions and create columns that indicate the section and barcode from the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "7326d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result = pca_df(result_s0)\n",
    "pca_result.shape\n",
    "pca_result['CELL'] = result_s0.index\n",
    "pca_result['ID'] = result_s0.index\n",
    "pca_result= pca_result.set_index(\"CELL\")\n",
    "pca_result[['SECTION','BARCODE']] = pca_result['ID'].str.split('-',expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab44b47a",
   "metadata": {},
   "source": [
    "Read the list of normal cells and add a variable for cell type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "ba41366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_cells = pd.read_csv('/scratch/projects/mudoublet/chisel-data/patientS0/normal_cells.txt', header=None, names=['CELL'])\n",
    "normal_cells = normal_cells.set_index(\"CELL\")\n",
    "normal_cells[\"TYPE\"] = \"NORMAL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cacd58e",
   "metadata": {},
   "source": [
    "Left join pca_result with normal cells data and replace NA in TYPE column wtih tumor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "a2b15d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result = pca_result.join(normal_cells, how=\"left\")\n",
    "pca_result = pca_result.fillna({\"TYPE\":\"TUMOR\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f1619",
   "metadata": {},
   "source": [
    "Preprocess the data first by inner-merging the labels into the data on #CHR, START, and END columns. Then, transform the data from wide to long with a column for each RDR and BAF and a row for a each cell. Use \"SECTION-BARCODE\" as the index. Drop the last two columns of result_s0 and result_s1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "3ca73c54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1311\n",
      "1233\n",
      "1311\n",
      "1019\n",
      "1153\n",
      "total cells:6027\n"
     ]
    }
   ],
   "source": [
    "finals = []\n",
    "totals = 0\n",
    "for d in datas:\n",
    "    datas[d]['#CHR'] = datas[d]['#CHR'].astype(str)  \n",
    "    datas[d] = datas[d].merge(labels, how='inner', on=['#CHR', 'START', 'END'])\n",
    "    x = create_rdr_baf(datas[d])\n",
    "    cell_id = d + \"-\" + datas[d]['CELL'].unique() \n",
    "    totals += cell_id.shape[0]\n",
    "    x = x.reset_index()\n",
    "    x = x.drop(columns=['CELL'])\n",
    "    x['CELL'] = cell_id\n",
    "    x = x.set_index(\"CELL\")\n",
    "    finals.append(x)               \n",
    "print(\"total cells:\" + str(totals))\n",
    "result_s1 = pd.concat(finals)\n",
    "result_s0 = result_s0.drop(columns=['15_15000000_20000000_x', '15_15000000_20000000_y'])\n",
    "result_s1 = result_s1.drop(columns=['15_15000000_20000000_x', '15_15000000_20000000_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28c478",
   "metadata": {},
   "source": [
    "### Mapping of Clone Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae79629",
   "metadata": {},
   "source": [
    "Read all the files from Clones into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "b862ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clones1 = pd.read_csv(\"patientS1/clones/sectionA1/mapping.tsv.gz\", sep=\"\\t\", compression=\"gzip\")\n",
    "clones2 = pd.read_csv(\"patientS1/clones/sectionA2/mapping.tsv.gz\", sep=\"\\t\", compression=\"gzip\")\n",
    "clones3 = pd.read_csv(\"patientS1/clones/sectionA3/mapping.tsv.gz\", sep=\"\\t\", compression=\"gzip\")\n",
    "clones4 = pd.read_csv(\"patientS1/clones/sectionB1/mapping.tsv.gz\", sep=\"\\t\", compression=\"gzip\")\n",
    "clones5 = pd.read_csv(\"patientS1/clones/sectionB2/mapping.tsv.gz\", sep=\"\\t\", compression=\"gzip\")\n",
    "clones = {'A1': clones1, 'A2': clones2,'A3': clones3,'B1': clones4,'B2': clones5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8301f1b",
   "metadata": {},
   "source": [
    "Create columns of section and cell while concatenating the clones into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "8456f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "em = []\n",
    "for c in clones:\n",
    "    temp = clones[c]\n",
    "    temp['SECTION'] = c\n",
    "    temp['CELL'] = temp['SECTION'] + '-' + temp['#CELL']\n",
    "    temp = temp.set_index('CELL')\n",
    "    em.append(temp)\n",
    "cloncal = pd.concat(em)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767556e7",
   "metadata": {},
   "source": [
    "Remove the clones containing None values to reduce errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "ee936ccc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "faults = cloncal['CLONE'] == 'None'\n",
    "faults = cloncal.loc[faults]  #Saves the clones with None values\n",
    "cloncals = cloncal[cloncal.CLONE != 'None'] #Removes \"None\" Clones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2ce2c",
   "metadata": {},
   "source": [
    "Identify the normal cells and set their index to the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "349392ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm1 = cloncals['CLONE'] == 'Clone68'\n",
    "norm1 = cloncals.loc[norm1]\n",
    "norm2 = cloncals['CLONE'] == 'Clone84'\n",
    "norm2 = cloncals.loc[norm2]\n",
    "norm3 = cloncals['CLONE'] == 'Clone9'\n",
    "norm3 = cloncals.loc[norm3]\n",
    "norm4 = cloncals['CLONE'] == 'Clone6'\n",
    "norm4 = cloncals.loc[norm4]\n",
    "norm5 = cloncals['CLONE'] == 'Clone11'\n",
    "norm5 = cloncals.loc[norm5]\n",
    "normss = {'t1': norm1, 't2': norm2,'t3': norm3,'t4': norm4,'t5': norm5}\n",
    "normss = pd.concat(normss, ignore_index=True)\n",
    "normss['CELL'] = normss['SECTION'] + '-' + normss['#CELL']\n",
    "normss = normss.set_index('CELL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb2c784",
   "metadata": {},
   "source": [
    "Fit and transform our result_s1 data to allow for further analysis and create our x_testfinal dataframe, which we will use to visualize our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b918c4",
   "metadata": {},
   "source": [
    "## Implementation of KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d282fedf",
   "metadata": {},
   "source": [
    "Create your x_train and x_test variables for the model, using the pcas_df() function and fit.transform method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "54caeb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, fit = pcas_df(result_s0, 2)\n",
    "x_test = fit.transform(result_s1)\n",
    "\n",
    "x_test = pd.DataFrame(data = x_test #s1\n",
    "             , columns = ['pca1', 'pca2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee10d92",
   "metadata": {},
   "source": [
    "Make a dataframe that contains the type of cells: normal or tumor cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "fe9bbdfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_test = x_test.set_index(result_s1.index)\n",
    "goodcells = x_test.index.difference(faults.index)\n",
    "x_testfinal = x_test.loc[goodcells]\n",
    "normss[\"TYPE\"] = \"normal\"\n",
    "x_testfinal = x_testfinal.join(normss, how=\"left\") #normas will be x_test when this works\n",
    "x_testfinal = x_testfinal.fillna({\"TYPE\":\"tumor\"})\n",
    "x_testfinal = x_testfinal.drop(columns=['#CELL', 'CLUSTER', 'CLONE', 'SECTION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a615798e",
   "metadata": {},
   "source": [
    "Implement the preprocessing for the types, creating an array with 1 corresponding to the tumor and 0 corresponding to a normal cell. Identify the amount of neighbors in the model and apply appropriate fitting. Find the accuracy of the test/training datas and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "831e4879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9965928449744463\n",
      "Train Accuracy: 0.9832385806704568\n"
     ]
    }
   ],
   "source": [
    "vn = preprocessing.LabelEncoder() #Allows for preprocessing\n",
    "cls = vn.fit_transform(list(pca_result[\"TYPE\"])) #1 is Tumor, 0 is Normal\n",
    "ycls = vn.fit_transform(list(x_testfinal['TYPE']))\n",
    "x_testa = x_testfinal[['pca1', 'pca2']].values\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(x_train, cls)\n",
    "\n",
    "acc_test = model.score(x_testa, ycls)\n",
    "acc_train = model.score(x_train, cls)\n",
    "print(\"Test Accuracy: \", acc_test) \n",
    "print(\"Train Accuracy:\", acc_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
